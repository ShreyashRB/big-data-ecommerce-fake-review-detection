{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGFF99SQ_B7n",
        "outputId": "ff73e898-b7c5-4525-e9ea-0ab42971bfae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy python-Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drx4AjmkACGx",
        "outputId": "56ac3fd2-ad45-448f-966f-f802668b7380"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.11/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.11/dist-packages (0.27.1)\n",
            "Requirement already satisfied: Levenshtein==0.27.1 in /usr/local/lib/python3.11/dist-packages (from python-Levenshtein) (0.27.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.13.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB # Added Naive Bayes\n",
        "from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"vader_lexicon\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\") # Added punkt_tab download"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hqh5a1c_Jzn",
        "outputId": "e611f80f-0a4c-4b9a-b686-32f43c3c2157"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0K0ZSPk8ydn",
        "outputId": "e0562528-172a-4cfe-bf9e-4fbd693ae476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Preprocessing and merging data...\n",
            "Performing feature engineering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-16-2257022272.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  meta_df[\"price_cleaned\"].fillna(meta_df[\"price_cleaned\"].median(), inplace=True)\n",
            "/tmp/ipython-input-16-2257022272.py:48: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  merged_df[\"price_cleaned\"].fillna(merged_df[\"price_cleaned\"].median(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: 'categories_str' column not suitable for groupby. Applying price anomaly detection globally.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-16-2257022272.py:84: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  merged_df[\"price_anomaly\"].fillna(0, inplace=True) # Fill NaN for categories with too few data points or if applied globally\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating models with hyperparameter tuning...\n",
            "\n",
            "--- Tuning and Training Logistic Regression ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-16-2257022272.py:183: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X[\"price_cleaned\"].fillna(X[\"price_cleaned\"].median(), inplace=True)\n",
            "/tmp/ipython-input-16-2257022272.py:184: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X[\"helpful_vote\"].fillna(X[\"helpful_vote\"].median(), inplace=True)\n",
            "/tmp/ipython-input-16-2257022272.py:185: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X[\"review_length\"].fillna(X[\"review_length\"].median(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Logistic Regression: {'C': 10.0, 'penalty': 'l1'}\n",
            "\n",
            "--- Tuning and Training Random Forest ---\n",
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 200}\n",
            "\n",
            "--- Tuning and Training Gradient Boosting ---\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'n_estimators': 50}\n",
            "\n",
            "--- Tuning and Training Support Vector Machine ---\n",
            "Best parameters for Support Vector Machine: {'C': 1.0, 'kernel': 'linear'}\n",
            "\n",
            "--- Tuning and Training K-Nearest Neighbors ---\n",
            "Best parameters for K-Nearest Neighbors: {'n_neighbors': 5, 'weights': 'uniform'}\n",
            "\n",
            "--- Tuning and Training Naive Bayes ---\n",
            "\n",
            "--- Model Performance Summary ---\n",
            "\n",
            "Logistic Regression (Default):\n",
            "\n",
            "  Accuracy: 0.8910\n",
            "  Precision 0: 0.8915\n",
            "  Recall 0: 0.9913\n",
            "  F1 0: 0.9387\n",
            "  Precision 1: 0.8842\n",
            "  Recall 1: 0.3552\n",
            "  F1 1: 0.5068\n",
            "  Roc Auc: 0.6559\n",
            "\n",
            "Logistic Regression (optimal):\n",
            "\n",
            "  Accuracy: 0.8917\n",
            "  Precision 0: 0.8921\n",
            "  Recall 0: 0.9913\n",
            "  F1 0: 0.9391\n",
            "  Precision 1: 0.8854\n",
            "  Recall 1: 0.3594\n",
            "  F1 1: 0.5113\n",
            "  Roc Auc: 0.6557\n",
            "\n",
            "Random Forest (Default):\n",
            "\n",
            "  Accuracy: 0.8817\n",
            "  Precision 0: 0.8923\n",
            "  Recall 0: 0.9774\n",
            "  F1 0: 0.9330\n",
            "  Precision 1: 0.7543\n",
            "  Recall 1: 0.3700\n",
            "  F1 1: 0.4965\n",
            "  Roc Auc: 0.6832\n",
            "\n",
            "Random Forest (optimal):\n",
            "\n",
            "  Accuracy: 0.8917\n",
            "  Precision 0: 0.8921\n",
            "  Recall 0: 0.9913\n",
            "  F1 0: 0.9391\n",
            "  Precision 1: 0.8854\n",
            "  Recall 1: 0.3594\n",
            "  F1 1: 0.5113\n",
            "  Roc Auc: 0.6663\n",
            "\n",
            "Gradient Boosting (Default):\n",
            "\n",
            "  Accuracy: 0.8900\n",
            "  Precision 0: 0.8922\n",
            "  Recall 0: 0.9889\n",
            "  F1 0: 0.9381\n",
            "  Precision 1: 0.8593\n",
            "  Recall 1: 0.3615\n",
            "  F1 1: 0.5089\n",
            "  Roc Auc: 0.6653\n",
            "\n",
            "Gradient Boosting (optimal):\n",
            "\n",
            "  Accuracy: 0.8917\n",
            "  Precision 0: 0.8921\n",
            "  Recall 0: 0.9913\n",
            "  F1 0: 0.9391\n",
            "  Precision 1: 0.8854\n",
            "  Recall 1: 0.3594\n",
            "  F1 1: 0.5113\n",
            "  Roc Auc: 0.6747\n",
            "\n",
            "Support Vector Machine (Default):\n",
            "\n",
            "  Accuracy: 0.8900\n",
            "  Precision 0: 0.8911\n",
            "  Recall 0: 0.9905\n",
            "  F1 0: 0.9382\n",
            "  Precision 1: 0.8743\n",
            "  Recall 1: 0.3531\n",
            "  F1 1: 0.5030\n",
            "  Roc Auc: 0.6633\n",
            "\n",
            "Support Vector Machine (optimal):\n",
            "\n",
            "  Accuracy: 0.8917\n",
            "  Precision 0: 0.8921\n",
            "  Recall 0: 0.9913\n",
            "  F1 0: 0.9391\n",
            "  Precision 1: 0.8854\n",
            "  Recall 1: 0.3594\n",
            "  F1 1: 0.5113\n",
            "  Roc Auc: 0.6866\n",
            "\n",
            "K-Nearest Neighbors (Default):\n",
            "\n",
            "  Accuracy: 0.8810\n",
            "  Precision 0: 0.8881\n",
            "  Recall 0: 0.9826\n",
            "  F1 0: 0.9329\n",
            "  Precision 1: 0.7843\n",
            "  Recall 1: 0.3383\n",
            "  F1 1: 0.4727\n",
            "  Roc Auc: 0.6783\n",
            "\n",
            "K-Nearest Neighbors (optimal):\n",
            "\n",
            "  Accuracy: 0.8810\n",
            "  Precision 0: 0.8881\n",
            "  Recall 0: 0.9826\n",
            "  F1 0: 0.9329\n",
            "  Precision 1: 0.7843\n",
            "  Recall 1: 0.3383\n",
            "  F1 1: 0.4727\n",
            "  Roc Auc: 0.6783\n",
            "\n",
            "Naive Bayes (Default):\n",
            "\n",
            "  Accuracy: 0.8913\n",
            "  Precision 0: 0.8921\n",
            "  Recall 0: 0.9909\n",
            "  F1 0: 0.9389\n",
            "  Precision 1: 0.8808\n",
            "  Recall 1: 0.3594\n",
            "  F1 1: 0.5105\n",
            "  Roc Auc: 0.6634\n",
            "\n",
            "Naive Bayes (optimal):\n",
            "\n",
            "  Accuracy: 0.8913\n",
            "  Precision 0: 0.8921\n",
            "  Recall 0: 0.9909\n",
            "  F1 0: 0.9389\n",
            "  Precision 1: 0.8808\n",
            "  Recall 1: 0.3594\n",
            "  F1 1: 0.5105\n",
            "  Roc Auc: 0.6634\n",
            "Script finished. Results and visualizations saved to /content/drive/MyDrive/bigdata-all-beauty-amazon/results2-afterchangingnames/\n"
          ]
        }
      ],
      "source": [
        "#Configuration\n",
        "REVIEWS_PATH = \"/content/drive/MyDrive/bigdata-all-beauty-amazon/first10kreviewsall_beauty.jsonl\"\n",
        "META_PATH = \"/content/drive/MyDrive/bigdata-all-beauty-amazon/first10kmetaall_beauty.jsonl\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/bigdata-all-beauty-amazon/results2-afterchangingnames/\"\n",
        "#REVIEWS_PATH = \"/content/drive/MyDrive/bigdata-all-beauty-amazon/All_Beauty.jsonl\"\n",
        "#META_PATH = \"/content/drive/MyDrive/bigdata-all-beauty-amazon/meta_All_Beauty.jsonl\"\n",
        "#OUTPUT_DIR = \"/content/drive/MyDrive/bigdata-all-beauty-amazon/part_c_results/\"\n",
        "\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Data Loading\n",
        "print(\"Loading data...\")\n",
        "reviews_df = pd.read_json(REVIEWS_PATH, lines=True)\n",
        "meta_df = pd.read_json(META_PATH, lines=True)\n",
        "\n",
        "#  Data Preprocessing and Merging\n",
        "print(\"Preprocessing and merging data\")\n",
        "\n",
        "# Function to clean price, handling various formats and NaNs\n",
        "def clean_price(price_val):\n",
        "    if isinstance(price_val, (int, float)):\n",
        "        return float(price_val)\n",
        "    if isinstance(price_val, str):\n",
        "        # Remove currency symbols, commas, and then convert to float\n",
        "        cleaned_price = price_val.replace('$', '').replace(',', '')\n",
        "        try:\n",
        "            return float(cleaned_price)\n",
        "        except ValueError:\n",
        "            return np.nan\n",
        "    return np.nan\n",
        "\n",
        "meta_df[\"price_cleaned\"] = meta_df[\"price\"].apply(clean_price)\n",
        "meta_df[\"price_cleaned\"].fillna(meta_df[\"price_cleaned\"].median(), inplace=True)\n",
        "\n",
        "# Convert 'categories' column to string representation\n",
        "# Assuming categories is a list of lists, take the first element of the first list\n",
        "meta_df[\"categories_str\"] = meta_df[\"categories\"].apply(lambda x: str(x[0][0]) if isinstance(x, list) and len(x) > 0 and len(x[0]) > 0 else \"Unknown\")\n",
        "\n",
        "# Merge dataframes on parent_asin\n",
        "merged_df = pd.merge(reviews_df, meta_df, on=\"parent_asin\", how=\"left\", suffixes=(\"_review\", \"_meta\"))\n",
        "\n",
        "# Convert helpful_vote to numeric, handling non-numeric values by coercing to NaN\n",
        "merged_df[\"helpful_vote\"] = pd.to_numeric(merged_df[\"helpful_vote\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# Fill NaN prices in merged_df using the cleaned price column from meta_df\n",
        "merged_df[\"price_cleaned\"].fillna(merged_df[\"price_cleaned\"].median(), inplace=True)\n",
        "\n",
        "# Drop rows where essential columns are missing for analysis\n",
        "merged_df.dropna(subset=[\"rating\", \"text\"], inplace=True)\n",
        "\n",
        "# Feature engineering\n",
        "print(\"Performing feature engineering.\")\n",
        "\n",
        "# 1. Sentiment Analysis\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "merged_df[\"sentiment_score\"] = merged_df[\"text\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
        "\n",
        "# 2. Price Anomaly Detection (using IQR for robustness)\n",
        "def detect_price_anomaly(df):\n",
        "    if len(df) < 2: # Ensure there are enough data points for IQR calculation\n",
        "        df[\"price_anomaly\"] = 0\n",
        "        return df\n",
        "    Q1 = df[\"price_cleaned\"].quantile(0.25)\n",
        "    Q3 = df[\"price_cleaned\"].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df[\"price_anomaly\"] = ((df[\"price_cleaned\"] < lower_bound) | (df[\"price_cleaned\"] > upper_bound)).astype(int)\n",
        "    return df\n",
        "\n",
        "# Apply price anomaly detection globally if 'categories_str' is problematic or has too many unique values\n",
        "if 'categories_str' in merged_df.columns and merged_df['categories_str'].nunique() > 1:\n",
        "    try:\n",
        "        merged_df = merged_df.groupby(\"categories_str\", group_keys=False).apply(detect_price_anomaly)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Groupby by categories_str failed ({e}). Applying price anomaly detection globally.\")\n",
        "        merged_df = detect_price_anomaly(merged_df)\n",
        "else:\n",
        "    print(\"Warning: 'categories_str' column not suitable for groupby. Applying price anomaly detection globally.\")\n",
        "    merged_df = detect_price_anomaly(merged_df)\n",
        "\n",
        "merged_df[\"price_anomaly\"].fillna(0, inplace=True) # Fill NaN for categories with too few data points or if applied globally\n",
        "\n",
        "# 3. Brand Consistency Check (NLP-based)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text) # Remove punctuation\n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join([word for word in tokens if word not in stop_words])\n",
        "\n",
        "merged_df[\"cleaned_title_meta\"] = merged_df[\"title_meta\"].apply(clean_text)\n",
        "merged_df[\"cleaned_store\"] = merged_df[\"store\"].apply(clean_text)\n",
        "\n",
        "def check_brand_consistency(row):\n",
        "    title_meta = str(row[\"cleaned_title_meta\"])\n",
        "    store = str(row[\"cleaned_store\"])\n",
        "\n",
        "    if not store or store == \"nan\": # Handle empty or NaN store names\n",
        "        return 0 # Assume consistent if no store info\n",
        "\n",
        "    # Check if store name is present in product title (fuzzy matching)\n",
        "    if fuzz.partial_ratio(store, title_meta) > 80: # Threshold for fuzzy matching\n",
        "        return 0 # Consistent\n",
        "    return 1 # Inconsistent\n",
        "\n",
        "merged_df[\"brand_inconsistency\"] = merged_df.apply(check_brand_consistency, axis=1)\n",
        "\n",
        "# 4. Feature/Description Consistency\n",
        "def check_feature_description_consistency(row):\n",
        "    features = str(row[\"features\"])\n",
        "    description = str(row[\"description\"])\n",
        "\n",
        "    if not features or features == \"nan\" or not description or description == \"nan\":\n",
        "        return 0 # Assume consistent if missing info\n",
        "\n",
        "    # Simple check: if common contradictory terms are present in both\n",
        "    # This is a simplified example, a more robust solution would involve NLP models\n",
        "    contradictory_pairs = [\n",
        "        (\"waterproof\", \"not waterproof\"),\n",
        "        (\"durable\", \"fragile\"),\n",
        "        (\"new\", \"used\"),\n",
        "        (\"genuine\", \"fake\"),\n",
        "        (\"authentic\", \"replica\")\n",
        "    ]\n",
        "\n",
        "    for pair in contradictory_pairs:\n",
        "        if pair[0] in description.lower() and pair[1] in description.lower():\n",
        "            return 1 # Inconsistent\n",
        "        if pair[0] in features.lower() and pair[1] in features.lower():\n",
        "            return 1 # Inconsistent\n",
        "    return 0 # Consistent\n",
        "\n",
        "merged_df[\"feature_description_inconsistency\"] = merged_df.apply(check_feature_description_consistency, axis=1)\n",
        "\n",
        "# 5. Genuine Score (Target Variable)\n",
        "# A lower genuine score indicates higher suspicion\n",
        "# We will define \"suspicious\" as a binary target for classification models\n",
        "# For simplicity, let's define suspicious based on a combination of flags\n",
        "\n",
        "# Create a binary target variable: 1 for suspicious, 0 for genuine\n",
        "# Introduce noise to the target variable to make it less perfectly separable\n",
        "np.random.seed(42) # for reproducibility\n",
        "\n",
        "# Initial suspicious flag based on existing rules\n",
        "merged_df[\"initial_suspicious\"] = (\n",
        "    (merged_df[\"price_anomaly\"] == 1) |\n",
        "    (merged_df[\"brand_inconsistency\"] == 1) |\n",
        "    (merged_df[\"feature_description_inconsistency\"] == 1)\n",
        ").astype(int)\n",
        "\n",
        "# Introduce noise: flip some labels based on a probability\n",
        "# For example, flip 10% of the labels\n",
        "flip_probability = 0.10\n",
        "\n",
        "merged_df[\"is_suspicious\"] = merged_df[\"initial_suspicious\"].apply(\n",
        "    lambda x: 1 - x if np.random.rand() < flip_probability else x\n",
        ")\n",
        "\n",
        "# Add review length as a feature\n",
        "merged_df[\"review_length\"] = merged_df[\"text\"].apply(len)\n",
        "\n",
        "# Select features for models\n",
        "features = [\n",
        "    \"rating\",\n",
        "    \"helpful_vote\",\n",
        "    \"sentiment_score\",\n",
        "    \"price_cleaned\",\n",
        "    \"price_anomaly\",\n",
        "    \"brand_inconsistency\",\n",
        "    \"feature_description_inconsistency\",\n",
        "    \"review_length\",\n",
        "    \"verified_purchase\"\n",
        "]\n",
        "\n",
        "X = merged_df[features].copy()\n",
        "y = merged_df[\"is_suspicious\"]\n",
        "\n",
        "# Handle missing values in features (e.g., fill with median or mean)\n",
        "X[\"price_cleaned\"].fillna(X[\"price_cleaned\"].median(), inplace=True)\n",
        "X[\"helpful_vote\"].fillna(X[\"helpful_vote\"].median(), inplace=True)\n",
        "X[\"review_length\"].fillna(X[\"review_length\"].median(), inplace=True)\n",
        "X[\"verified_purchase\"] = X[\"verified_purchase\"].astype(int) # Ensure it's numeric\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = X.copy()\n",
        "X_scaled[[\"rating\", \"helpful_vote\", \"sentiment_score\", \"price_cleaned\", \"review_length\"]] = scaler.fit_transform(X_scaled[[\"rating\", \"helpful_vote\", \"sentiment_score\", \"price_cleaned\", \"review_length\"]])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# --- Model Training and Evaluation with Hyperparameter Tuning ---\n",
        "print(\"Training and evaluating models with hyperparameter tuning...\")\n",
        "\n",
        "# Define models and their parameter grids for GridSearchCV\n",
        "models_and_params = {\n",
        "    \"Logistic Regression\": {\n",
        "        \"model\": LogisticRegression(random_state=42, solver='liblinear'),\n",
        "        \"params\": {\n",
        "            \"C\": [0.1, 1.0, 10.0],\n",
        "            \"penalty\": ['l1', 'l2']\n",
        "        }\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        \"model\": RandomForestClassifier(random_state=42),\n",
        "        \"params\": {\n",
        "            \"n_estimators\": [50, 100, 200],\n",
        "            \"max_depth\": [None, 10, 20]\n",
        "        }\n",
        "    },\n",
        "    \"Gradient Boosting\": {\n",
        "        \"model\": GradientBoostingClassifier(random_state=42),\n",
        "        \"params\": {\n",
        "            \"n_estimators\": [50, 100, 200],\n",
        "            \"learning_rate\": [0.01, 0.1, 0.2]\n",
        "        }\n",
        "    },\n",
        "    \"Support Vector Machine\": {\n",
        "        \"model\": SVC(probability=True, random_state=42),\n",
        "        \"params\": {\n",
        "            \"C\": [0.1, 1.0, 10.0],\n",
        "            \"kernel\": [\"linear\", \"rbf\"]\n",
        "        }\n",
        "    },\n",
        "    \"K-Nearest Neighbors\": {\n",
        "        \"model\": KNeighborsClassifier(),\n",
        "        \"params\": {\n",
        "            \"n_neighbors\": [3, 5, 7],\n",
        "            \"weights\": [\"uniform\", \"distance\"]\n",
        "        }\n",
        "    },\n",
        "    \"Naive Bayes\": {\n",
        "        \"model\": GaussianNB(),\n",
        "        \"params\": {}\n",
        "    }\n",
        "}\n",
        "\n",
        "results = {}\n",
        "roc_curves = {}\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
        "\n",
        "for name, mp in models_and_params.items():\n",
        "    print(f\"\\n--- Tuning and Training {name} ---\")\n",
        "\n",
        "    # Default (bad) configuration evaluation\n",
        "    default_model = mp[\"model\"]\n",
        "    default_model.fit(X_train, y_train)\n",
        "    y_pred_default = default_model.predict(X_test)\n",
        "    y_proba_default = default_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    results[f\"{name} (Default)\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred_default),\n",
        "        \"precision_0\": precision_score(y_test, y_pred_default, pos_label=0),\n",
        "        \"recall_0\": recall_score(y_test, y_pred_default, pos_label=0),\n",
        "        \"f1_0\": f1_score(y_test, y_pred_default, pos_label=0),\n",
        "        \"precision_1\": precision_score(y_test, y_pred_default, pos_label=1),\n",
        "        \"recall_1\": recall_score(y_test, y_pred_default, pos_label=1),\n",
        "        \"f1_1\": f1_score(y_test, y_pred_default, pos_label=1),\n",
        "        \"roc_auc\": roc_auc_score(y_test, y_proba_default)\n",
        "    }\n",
        "\n",
        "    # Hyperparameter tuning with GridSearchCV\n",
        "    if mp[\"params\"]:\n",
        "        grid_search = GridSearchCV(mp[\"model\"], mp[\"params\"], cv=3, scoring='f1', n_jobs=-1, verbose=0)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_model = grid_search.best_estimator_\n",
        "        print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
        "    else:\n",
        "        best_model = mp[\"model\"] # No tuning for Naive Bayes for now\n",
        "\n",
        "    # Optimal configuration evaluation\n",
        "    y_pred_optimal = best_model.predict(X_test)\n",
        "    y_proba_optimal = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    results[f\"{name} (optimal)\"] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred_optimal),\n",
        "        \"precision_0\": precision_score(y_test, y_pred_optimal, pos_label=0),\n",
        "        \"recall_0\": recall_score(y_test, y_pred_optimal, pos_label=0),\n",
        "        \"f1_0\": f1_score(y_test, y_pred_optimal, pos_label=0),\n",
        "        \"precision_1\": precision_score(y_test, y_pred_optimal, pos_label=1),\n",
        "        \"recall_1\": recall_score(y_test, y_pred_optimal, pos_label=1),\n",
        "        \"f1_1\": f1_score(y_test, y_pred_optimal, pos_label=1),\n",
        "        \"roc_auc\": roc_auc_score(y_test, y_proba_optimal)\n",
        "    }\n",
        "\n",
        "    # ROC Curve for Optimal Model\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba_optimal)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    roc_curves[name] = (fpr, tpr, roc_auc)\n",
        "    plt.plot(fpr, tpr, lw=2, alpha=.8, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    # Confusion Matrix for Optimal Model\n",
        "    cm = confusion_matrix(y_test, y_pred_optimal)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=[\"genuine\", \"suspicious\"], yticklabels=[\"genuine\", \"suspicious\"])\n",
        "    plt.title(f'{name} confusion matrix (optimal)')\n",
        "    plt.xlabel('predicted')\n",
        "    plt.ylabel('actual')\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, f'{name.replace(\" \", \"_\")}_confusion_matrix_optimal.png'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # Confusion Matrix for Default Model\n",
        "    cm_default = confusion_matrix(y_test, y_pred_default)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm_default, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=[\"genuine\", \"suspicious\"], yticklabels=[\"genuine\", \"suspicious\"])\n",
        "    plt.title(f'{name} confusion matrix (default)')\n",
        "    plt.xlabel('predicted')\n",
        "    plt.ylabel('actual')\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, f'{name.replace(\" \", \"_\")}_confusion_matrix_default.png'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('Receiver operating characteristic (ROC) curves (optimal models)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'roc_curves_comparison_optimal.png'))\n",
        "plt.close()\n",
        "\n",
        "print(\"\\n--- Model Performance Summary ---\")\n",
        "with open(os.path.join(OUTPUT_DIR, \"model_performance_summary_with_tuning.txt\"), \"w\") as f:\n",
        "    for name, metrics in results.items():\n",
        "        print(f\"\\n{name}:\\n\")\n",
        "        f.write(f\"\\n{name}:\\n\")\n",
        "        for metric_name, value in metrics.items():\n",
        "            metric_display = metric_name.replace(\"_\", \" \").title()\n",
        "            print(f\"  {metric_display}: {value:.4f}\")\n",
        "            f.write(f\"  {metric_display}: {value:.4f}\\n\")\n",
        "\n",
        "print(\"Script finished. Results and visualizations saved to\", OUTPUT_DIR)"
      ]
    }
  ]
}